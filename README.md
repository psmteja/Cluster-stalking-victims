| Rank | Model            | Best CV Macro-F1 | Best Params                                                                                   |
| ---: | ---------------- | ---------------: | --------------------------------------------------------------------------------------------- |
|    1 | **MLP**          |     **0.308294** | `{'mlpclassifier__alpha': 0.01, 'mlpclassifier__hidden_layer_sizes': (…)}`                    |
|    2 | **BernoulliNB**  |     **0.299671** | `{'bernoullinb__alpha': 0.1}`                                                                 |
|    3 | **DecisionTree** |     **0.279022** | `{'decisiontreeclassifier__max_depth': None, 'decisiontreeclassifier__min_samples_split': …}` |
|    4 | **GaussianNB**   |     **0.252555** | `{'gaussiannb__var_smoothing': 1e-07}`                                                        |
|    5 | **LogReg_L2**    |     **0.225567** | `{'logisticregression__C': 10.0}`                                                             |


Notes:
| Model Name       | Full Name                                                    | Description                                                                                                                                                   | Key Parameters / Notes                                                                                                      |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| **MLP**          | *Multilayer Perceptron (Artificial Neural Network)*          | A feed-forward neural network that captures complex, nonlinear relationships between predictors and the target. It learns through backpropagation.            | `hidden_layer_sizes`, `alpha` (L2 regularization strength). The best model here had `alpha=0.01` and one/two hidden layers. |
| **LogReg_L2**    | *Logistic Regression with L2 penalty (Ridge Regularization)* | A linear model that predicts probabilities using a logistic (sigmoid) function. The **L2** penalty helps prevent overfitting by shrinking large coefficients. | Parameter `C` controls regularization strength (smaller = stronger regularization).                                         |
| **LogReg_L1**    | *Logistic Regression with L1 penalty (Lasso Regularization)* | Similar to LogReg_L2, but uses an **L1** penalty that can drive some coefficients exactly to zero—useful for feature selection.                               | Parameter `C` controls regularization strength.                                                                             |
| **DecisionTree** | *Decision Tree Classifier*                                   | A non-parametric model that recursively splits the data on features to maximize purity of resulting groups. Interpretable but prone to overfitting.           | Tuned over `max_depth` and `min_samples_split`.                                                                             |
| **GaussianNB**   | *Gaussian Naïve Bayes*                                       | A probabilistic classifier assuming normally distributed features within each class. Simple and fast.                                                         | Tuned via `var_smoothing` to control numeric stability.                                                                     |
| **BernoulliNB**  | *Bernoulli Naïve Bayes*                                      | Similar to GaussianNB, but designed for binary features (0/1). Suitable for dichotomous survey variables.                                                     | Tuned via `alpha` (Laplace smoothing).                                                                                      |
